{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPC\n",
    "\n",
    "**Table of contents**\n",
    "* [Pending classes to review](#Pending-classes-to-review)\n",
    "* [Class 008 - Parallel code scalability](#Class-008-Parallel-code-scalability)\n",
    "  * [Synchronicity](#Synchronicity)\n",
    "  * [Flynn's taxonomy](#Flynn's-taxonomy)\n",
    "  * [Types of memory systems](#Types-of-memory-systems)\n",
    "  * [Speedup and efficiency](#Speedup-and-efficiency)\n",
    "  * [Amdahl's Law](#Amdahl's-Law)\n",
    "  * [Gustafon's Law](#Gustafson's-Law)\n",
    "  * [Weak scalability vs. Strong scalability](#Weak-scalability-vs.-Strong-scalability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pending classes to review\n",
    "\n",
    "* Class 007 - Code Profiling\n",
    "* Class 006 - Measuring time, experimental errors and reproducibility\n",
    "* Class 005 - Git, Github and TravisCI\n",
    "* Class 004 - Reproducibility, compilers and autobuilders\n",
    "* Class 003 - Containers\n",
    "* Class 002 - History of processors development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 008 - Parallel code scalability\n",
    "\n",
    "> **Date:** 2021-04-17  \n",
    "> **Subjects:**\n",
    "> * [Synchronicity](#Synchronicity)\n",
    "> * [Flynn's taxonomy](#Flynn's-taxonomy)\n",
    "> * [Types of memory systems](#Types-of-memory-systems)\n",
    "> * [Speedup and efficiency](#Speedup-and-efficiency)\n",
    "> * [Amdahl's Law](#Amdahl's-Law)\n",
    "> * [Gustafon's Law](#Gustafson's-Law)\n",
    "> * [Weak scalability vs. Strong scalability](#Weak-scalability-vs.-Strong-scalability)\n",
    "> \n",
    "> **References:**\n",
    "> * [Recorded class - First part][class-recording-01]\n",
    "> * [Recorded class - Second part][class-recording-02]\n",
    "> * [Slides][class-slides]\n",
    "\n",
    "[class-recording-01]: https://drive.google.com/file/d/1Eqq6bTxox54uH1siH--NqDBNnSWvxvEi/view?usp=sharing\n",
    "[class-recording-02]: https://drive.google.com/file/d/1RzKEi6CAXYNo-GfJ63VjQSm5SgzAfBzX/view?usp=sharing\n",
    "[class-slides]: https://drive.google.com/file/d/1I5Vkz9oI0KCFGw-YgP-B1G8TFz88vTqB/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronicity\n",
    "\n",
    "While executing a given program, you can have two types of operations:\n",
    "\n",
    "1. **Synchronous operations:** These operations are those that can happen at the same time. So, they can be parallelized\n",
    "2. **Asynchronous operations:** These operations are not happening at the same time\n",
    "\n",
    "During the execution of a given program, we can have some synchronization operations (like a **barrier operation**) that could synchronize our code for us. These are extremelly helpful while coding a parallelizable system, because it is common that some operations can happen synchronously, but other don't.\n",
    "\n",
    "The barrier operations can synchrone multiple **threads** for the same point. For example, we can have the following operations:\n",
    "\n",
    "$$ T_1 \\longrightarrow T_3 \\longrightarrow T_5$$\n",
    "\n",
    "If the $T_5$ operations depends on the $T_3$ result, we need to add a barrier before that if we want to parallelize our code. Now we have:\n",
    "\n",
    "$$ T_1 \\longrightarrow T_3 \\longrightarrow B \\longrightarrow T_5$$\n",
    "\n",
    "Being $B$ the barrier operation.\n",
    "\n",
    "Now, we can safely execute our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flynn's taxonomy\n",
    "\n",
    "The **Flynn's taxonomy** aims to standardize the possible scenarios of a given code execution. It has three types os execution scenarios:\n",
    "\n",
    "#### 1. SISD (Single instruction stream, single data stream)\n",
    "\n",
    "This is the classical, von Neumann, scenario. Here, we have a sequential execution, where only a single process is being executed and receiving inputs from a single data stream.\n",
    "\n",
    "<img src=\"./assets/hpc-sisd-diagram.png\" width=\"250px\" />\n",
    "\n",
    "#### 2. SIMD (Single instruction stream, multiple data stream)\n",
    "\n",
    "In this scenario, we have multiple operations being execution for each instruction.\n",
    "\n",
    "<img src=\"./assets/hpc-simd-diagram.png\" />\n",
    "\n",
    "#### 3. MISD (Multiple instruction stream, single data stream)\n",
    "\n",
    "In this scenario, we have just a single data stream, but being processed by multiple instruction streams\n",
    "\n",
    "<img src=\"./assets/hpc-misd-diagram.png\" width=\"250px\" />\n",
    "\n",
    "#### 4. MIMD (Multiple instruction stream, multiple data stream)\n",
    "\n",
    "In this scenario we have both data and instruction flow independents. So, we have multiple data streams, and also multiple threads executing those.\n",
    "\n",
    "<img src=\"./assets/hpc-mimd-diagram.png\" width=\"250px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of memory systems\n",
    "\n",
    "In a given application, we can have two types of memory systems:\n",
    "\n",
    "1. **Shared-memory system:** This happens when all proccess in that program are sharing the same memory. Usually, when we want to parallelize these programs, we need to use a given API like [OpenMP](https://www.openmp.org/)\n",
    "2. **Distributive-memory systems:** These type of systems are when your proccesses are running in different memory points. *Usually* this happens when they are phisically separated (different nodes), **but that is not always the case**. Some computers can have separate memory points for different instances of a given program *in the same node*. In this type of system, the **network** is extremelly important. Because we need to use a message API (like [MPI](https://www.open-mpi.org)) to synchronize our nodes.\n",
    "\n",
    "One important thing to note is **latency**. When we have the second system type, we need to watch out for latency. So, we need a good performance in our network layer.\n",
    "\n",
    "Based on this, we've created the **BSP (Bulk synchronous parallel) programming model**. That model aimed to solve how we code synchronous systems, by creating a standardized way to do so. This was introduced [in an article](https://dl.acm.org/doi/10.1145/79173.79181). That model is organized in **rounds** where, in each round, all processes do the following:\n",
    "\n",
    "1. Do the computation locally\n",
    "2. Send messages for all other processes\n",
    "3. Way all messages to be received, waiting a am given barrier until the next round, when all proccesses start together\n",
    "\n",
    "Here, we have two important keywords to note:\n",
    "\n",
    "* **Overhead:** Which is the excess or indirect computation time, memory, or anything that are required to perform a specific task\n",
    "* **Latency:** Which is the amount of time waiting our network\n",
    "* **Bottlenecks:** Which is basically which part of our system is overloaded\n",
    "\n",
    "There are multiple types of bottlenecks, some common are:\n",
    "\n",
    "* **IO bound:** When we can't increase our performance, since our IO device is overloaded\n",
    "* **CPU bound:** When we can't increase our performance, since our CPU is overloaded\n",
    "* **Memory bound:** Same as above\n",
    "\n",
    "Basically, we can have *anything* bound as a type of bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speedup and efficiency\n",
    "\n",
    "#### Speedup\n",
    "\n",
    "It is a metric that measures the increase of performance for a given proccess when we parallelize it. Usually it uses the following notations:\n",
    "\n",
    "$$\n",
    "T_{s} = \\text{Serial (not parallel) execution time} \\\\\n",
    "T_{p} = \\text{Parallel execution time}\n",
    "S = \\text{Speedup}\n",
    "$$\n",
    "\n",
    "We can calculate the speedup with following formula:\n",
    "\n",
    "$$\n",
    "S = \\dfrac{T_s}{T_p}\n",
    "$$\n",
    "\n",
    "#### Efficiency\n",
    "\n",
    "Measures the efficiency of our parallel proccess. It is calculated by the following formula:\n",
    "\n",
    "$$\n",
    "E = \\dfrac{S(p)}{p}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $E$: Efficiency\n",
    "* $S$: Speedup\n",
    "* $p$: Amount of parallel resources\n",
    "\n",
    "Based on this, we can state that it is possible to have a **linear speedup**. We can state that when this statement is `true`:\n",
    "\n",
    "$$\n",
    "S(p) = 1 \\land E = 1\n",
    "$$\n",
    "\n",
    "In this scenario (both speedup and effiency are $1$), we can say that we can decrease our execution time at the same rate as we add new parallel resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amdahl's Law\n",
    "\n",
    "This is a law that was created based on the observations of [this article](https://dl.acm.org/doi/10.1145/1465482.1465560). That law calculates the parallel execution time of a given process.\n",
    "\n",
    "Considering:\n",
    "* $T_p$ = The parallel execution time\n",
    "* $T_s$ = The serial execution time\n",
    "* $r$ = The fraction of the program that can be parallelized\n",
    "* $p$ = Number of parallel resources\n",
    "\n",
    "This law states that:\n",
    "$$\n",
    "T_p = (1 - r) \\times T_s + \\dfrac{(r \\times T_s)}{p}\n",
    "$$\n",
    "\n",
    "So, when we calculate the limit of that execution time:\n",
    "$$\n",
    "\\lim_{p \\to +\\infty} T_P = (1-r) \\times T_s\n",
    "$$\n",
    "\n",
    "Passing it to the `Speedup` and `Efficiency`:\n",
    "\n",
    "$$\n",
    "\\lim_{p \\to +\\infty} S = \\dfrac{T_s}{(1 - r) \\times T_s} = \\dfrac{1}{1 - r} \\\\\n",
    "$$\n",
    "$$\n",
    "\\lim_{p \\to +\\infty} E = 0\n",
    "$$\n",
    "\n",
    "Based on this law, we can state that the maximum performance of a given program drastically changes based on how much that program is parallelizable. We can have a lot of examples like the following:\n",
    "\n",
    "* If a given program is 10% serial\n",
    "* Our max speedup is 10 times\n",
    "\n",
    "So, in this case, the maximum performance is 10 times. We usually consider this as a hard definition, since it doesn't consider the problem issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gustafson's Law\n",
    "\n",
    "This law was proposed in [this article](https://dl.acm.org/doi/10.1145/42411.42415), where Gustavson analyses the Amdahl's Law, and states that it simply ignores the size of the problem our program is dealing with. In a nutshell, it highlights that Amdahl's Law compares the execution time of the same algorithm using the same problem size, comparing different amount of parallel resources, but even if Amdahl's Law states that a given number is our maximum speedup, that number could be bigger if we increase the size of our problem.\n",
    "\n",
    "<img src=\"./assets/hpc-amdahl-vs-gutafson.gif\" />\n",
    "\n",
    "In Gustafson's Law, we consider the parallel portion of the code as $r$ and we multiple that $r$ amount in our serial comparisson by the number of parallel resources, for example:\n",
    "\n",
    "<img src=\"./assets/hpc-gustafson-graphical-representation.png\" />\n",
    "\n",
    "So, for Gutafson, we have the following:\n",
    "\n",
    "$$\n",
    "S = 1 - r + r \\times p \\\\\n",
    "E = \\dfrac{r + (1 - r)}{p}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $S$ = Scaled speedup\n",
    "* $r$ = Percentual portion of the program that can be parallelized\n",
    "* $p$ = Amount of parallel resources\n",
    "* $E$ = Efficiency\n",
    "\n",
    "If we analyse it in infinity, we have:\n",
    "\n",
    "$$\n",
    "\\lim_{p \\to +\\infty} S = r \\times p \\\\\n",
    "\\lim_{p \\to +\\infty} E = \\sim r\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak scalability vs. Strong scalability\n",
    "\n",
    "Scalability is an extremelly common topic in computer science. We can use two main sources to define it:\n",
    "* [This article](http://pages.cs.wisc.edu/~markhill/papers/can90_scalability.pdf)\n",
    "* [This book](https://www.amazon.com/Introduction-Parallel-Programming-Peter-Pacheco/dp/0123742609)\n",
    "\n",
    "Those sources define a scalable software as:\n",
    "> A scalable software can increase the amount of parallel resources to increase performance at the same rate.\n",
    "\n",
    "If the increase in the number of parallel resources has a relation with the increase of the problem with a constant efficiency, we can state that the program is scalable. Here is a simple example:\n",
    "\n",
    "<img src=\"./assets/hpc-scalable-chart-example.png\" />\n",
    "\n",
    "We can define two main types of scalability:\n",
    "\n",
    "1. **Strong scalability:** This happens when we keep the efficiency when we increase the parallelism without changing the problem size (Amdahl's based)\n",
    "2. **Weak scalability:** This happens when we keep the efficiency when we increase the parallelism and the problem size at the same rate (Gustafson's based)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.16 64-bit",
   "name": "python2716jvsc74a57bd0767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}